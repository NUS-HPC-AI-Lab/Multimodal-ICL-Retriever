hydra:
  job:
    chdir: false

model_name: 'openai/clip-vit-large-patch14'              # the model name of the LM inferencer
task_name: coco
output_file: coco_train/scorer.json

batch_size: 384                 # the batch_size of the model when using `hf-gen_a` model_config; for api models, the batch size is decided based on the number of openai keys.

vision_encoder_path: "ViT-L-14"
vision_encoder_pretrained: "openai"
lang_encoder_path: "anas-awadalla/mpt-1b-redpajama-200b"
tokenizer_path: "anas-awadalla/mpt-1b-redpajama-200b"

# parameters needed to initialize the inference dataset reader
dataset_reader:
  _target_: src.dataset_readers.scoring_dsr.ScorerDatasetReader
  dataset_path: coco_train/retrieved.json
  dataset_split: null
  ds_size: null
  task_name: ${task_name}
  n_ics: 1               # maximum number of shots as prompt
  field: gen_a           # this field will be used to construct prompt
  index_reader: ${index_reader}

# parameters needed to initialize the index reader
index_reader:
  _target_: src.dataset_readers.index_dsr.IndexDatasetReader
  task_name: ${task_name}
  model_name: ${model_name}
  field: qa                    # the field used for in-context examples, `qa` refers to the whole input-output pairs
  dataset_path: coco_train/index_dataset.json
  dataset_split: null
  ds_size: null